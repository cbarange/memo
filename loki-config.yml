# Used with docker grafana/loki:3.5
# DOC : https://grafana.com/docs/loki/latest/configure/

# --- Note ---
# - Three article you must read before :
#     Loki: Effective Logging and Log Aggregation with Grafana
#     https://medium.com/@gpiechnik/loki-effective-logging-and-log-aggregation-with-grafana-c3356e7f13ad
#     Grafana Loki Configuration Nuances
#     https://medium.com/lonto-digital-services-integrator/grafana-loki-configuration-nuances-2e9b94da4ac1
#     Grafana Loki â€” Our journey on replacing Elastic Search and adopting a new logging solution
#     https://medium.com/engenharia-arquivei/grafana-loki-our-journey-on-replacing-elastic-search-and-adopting-a-new-logging-solution-at-f65aec407e47
#
# - Data Insertion : 
#     https://grafana.com/docs/loki/latest/send-data/
#     You can insert Data with the HTTP API : https://medium.com/geekculture/pushing-logs-to-loki-without-using-promtail-fc31dfdde3c6
#     , with Promtail, with logstash(plugin: logstash-output-loki)
# 
#     Please note that you can't insert log with a timestamp older than the reject_old_samples_max_age
#     and if the stream already have logs/data (for a existing stream you can only insert logs/data 
#     with a timestamp not older than the most recent in the stream minus half of the  max_chunk_age(default:2h))
# 
# - LogQL :
#     LogQL is the query language of Loki you can install LogCLI to run you command or use the grafana loki connector
# 
# - Cluster :
#     You can use loki as microservice with a multiple instance(also named ring) mode. 
# --- === ---
auth_enabled: false

server:
  http_listen_address: 0.0.0.0
  http_listen_port: 3100 # Http port used to send data to loki 
  http_server_read_timeout: 180s # allow longer time span queries
  http_server_write_timeout: 180s # allow longer time span queries
  
  grpc_listen_address: 0.0.0.0
  grpc_listen_port: 9096 # GRPC port to send data to loki (have to publish this port in case of a docker environnement)
  grpc_server_max_recv_msg_size: 133554432 # 132MiB (int bytes), default 4MB
  grpc_server_max_send_msg_size: 133554432 # 132MiB (int bytes), default 4MB = 4194304
  
  log_level: info # Valid levels: [debug, info, warn, error], default info
  log_format: json # default "logfmt"


common:
  instance_addr: 127.0.0.1 # IP address to advertise in the ring in case of multi loki instance cluster
  path_prefix: /loki # Data directory ?
  # storage:
  #   filesystem: # Describe filesysteme config
  #     chunks_directory: /tmp/loki/chunks
  #     rules_directory: /tmp/loki/rules
      
  # storage:
  #   s3:
  #     endpoint: 127.0.0.1:9000
  #     bucketnames: loki
  #     access_key_id: admin
  #     secret_access_key: admin123
  #     insecure: true
  #     s3forcepathstyle: true

  replication_factor: 1 # The number of ingesters to write to and read from, the number of service that write and read data to the persistent storage, can be set to 3
  ring: # For multi loki instance cluster
    kvstore:
      store: inmemory # Backend storage to use for the ring.

# The query_range block configures the query splitting and caching in the Loki
query_range:
  parallelise_shardable_queries: false
  results_cache:
    cache:
      embedded_cache:
        enabled: true
        max_size_mb: 100

querier:
  max_concurrent: 16 # default = 4
  # query_timeout: 300s
  # engine:
  #   timeout: 300s

query_scheduler:
  max_outstanding_requests_per_tenant: 32768 # default = 32000

frontend:
  max_outstanding_per_tenant: 4096 # default = 2048
  compress_responses: true # default = true

# --------- STORAGE ---------
# Configures the chunk index schema and where it is stored.
# DOC : https://grafana.com/docs/loki/latest/configure/#schema_config
schema_config:
  configs:
    - from: 2020-10-24
      store: tsdb
      object_store: s3
      # object_store: filesystem
      schema: v13
      index:
        prefix: index_
        period: 24h

# --------- RETENTION ---------
# docs: https://grafana.com/docs/loki/latest/operations/storage/retention/

# DOC : https://grafana.com/docs/loki/latest/configure/#limits_config
limits_config:
  max_streams_per_user: 10000
  max_line_size: 256000
  max_entries_limit_per_query: 50000 # default 5000
  split_queries_by_interval: 0 # 24h
  query_timeout: 3m # default = 1m
  max_query_series: 200000 # default = 500
  max_query_length: 0 # The limit to length of chunk store queries. 0 to disable. default = 30d1h
  max_global_streams_per_user: 0 # Maximum number of active streams per user, across the cluster. 0 to disable. default = 5000
  ingestion_rate_mb: 50000 # Per-user ingestion rate limit in sample size per second. Units in MB. default = 4
  ingestion_burst_size_mb: 50000 # Per-user allowed ingestion burst size (in sample size). Units in MB. default = 6
  volume_enabled: true # Enable log-volume endpoints. default true
  reject_old_samples: true # default = true
  reject_old_samples_max_age: 10w # default = 1w
  per_stream_rate_limit: 5MB # Max Recommandation is 5MB
  per_stream_rate_limit_burst: 20MB # Max Recommandation is 20MB
  retention_period: 90d # 90 days which is applied globally.
  retention_stream: # if a stream matches multiple selector the highest priority is picked. In case multiple stream are matching, the highest priority will be picked. If no rule is matched the 'retention_period' is used.
  - selector: '{retention="30d"}' # label selection
    priority: 3
    period: 30d 
  - selector: '{retention="60d"}' # label selection
    priority: 4
    period: 60d 
  
# Stream matching label uses the same syntax as Prometheus label matching:
# =: Select labels that are exactly equal to the provided string.
# !=: Select labels that are not equal to the provided string.
# =~: Select labels that regex-match the provided string.
# !~: Select labels that do not regex-match the provided string.

# --------- OTHER ---------
ruler:
  alertmanager_url: http://localhost:9093

analytics:
 reporting_enabled: false

# Press Ctrl+D